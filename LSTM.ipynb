{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r78e5raX7Qq1"
      },
      "source": [
        "#Implementing the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0W9dPf8NV79"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BbH2lPhN4P3"
      },
      "source": [
        "data = open(\"sh.txt\").read().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFU_Jm1U7VTH",
        "outputId": "7642284e-6a50-4647-caa2-3bdd329234bd"
      },
      "source": [
        "data = data.rstrip()\n",
        "data = data[:5000]\n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMwnOR3O67D",
        "outputId": "99df9a07-3226-412f-f169-a1c54245db85"
      },
      "source": [
        "l = [\",\",\".\",\"?\",\"|\",\"(\",\")\"]\n",
        "\n",
        "for i in l:\n",
        "  data = data.replace(i,\"\")\n",
        "\n",
        "data = re.sub(\"(\\n+)\",\" \",data)\n",
        "data = data.split(\" \")\n",
        "\n",
        "chars = set(data)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "ch2id = {c : i for i,c in enumerate(chars)}\n",
        "id2ch = {i:c for i,c in enumerate(chars)}\n",
        "\n",
        "print(vocab_size)\n",
        "print(ch2id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "442\n",
            "{'': 0, \"summer's\": 1, 'refigured': 2, 'answer': 3, 'spend': 4, 'delights': 5, 'place': 6, 'feeble': 7, 'world': 8, 'deep': 9, \"th'\": 10, 'churl': 11, 'beauteous': 12, 'image': 13, 'back': 14, 'due': 15, 'sum': 16, 'play': 17, 'burning': 18, 'fair': 19, 'eyes': 20, 'son': 21, 'my': 22, 'ornament': 23, 'self': 24, 'all-eating': 25, 'each': 26, 'his': 27, 'leese': 28, 'serving': 29, 'climbed': 30, 'an': 31, 'very': 32, 'or': 33, 'frost': 34, 'lends': 35, 'in': 36, 'pay': 37, 'die': 38, 'womb': 39, 'only': 40, 'confounds': 41, 'by': 42, 'child': 43, 'ten': 44, 'posterity': 45, 'proud': 46, \"consum'st\": 47, 'flame': 48, 'making': 49, 'middle': 50, 'beauty': 51, 'and': 52, 'song': 53, 'stop': 54, 'fond': 55, 'windows': 56, \"winter's\": 57, 'way:': 58, 'gladly': 59, 'strikes': 60, 'bear': 61, 'highmost': 62, 'days;': 63, 'made': 64, 'lies': 65, 'how': 66, 'eye': 67, 'time': 68, \"excuse'\": 69, 'lovely': 70, 'usurer': 71, 'trenches': 72, 'see': 73, 'praise': 74, 'the': 75, 'youth': 76, 'held:': 77, 'ear': 78, 'sing:': 79, 'than': 80, 'but': 81, 'hours': 82, 'sunken': 83, 'she': 84, 'abuse': 85, 'though': 86, 'upon': 87, 'times': 88, 'niggard': 89, 'prime': 90, 'form': 91, 'waste': 92, 'pity': 93, 'tattered': 94, 'fairest': 95, 'renewest': 96, 'gaze': 97, \"death's\": 98, 'mortal': 99, 'make': 100, 'where': 101, 'no': 102, 'work': 103, 'gracious': 104, 'bequest': 105, 'hideous': 106, 'tyrants': 107, 'did': 108, 'converted': 109, 'homage': 110, 'like': 111, 'reeleth': 112, 'through': 113, 'it': 114, 'depart': 115, 'lusty': 116, 'vial;': 117, 'shame': 118, 'married': 119, \"nature's\": 120, 'are': 121, 'new': 122, 'that': 123, 'worms': 124, \"mak'st\": 125, 'gone': 126, 'note': 127, 'head': 128, 'left': 129, 'annoy': 130, 'april': 131, 'majesty': 132, 'up': 133, \"light's\": 134, 'loan;': 135, 'weary': 136, 'husbandry': 137, 'never-resting': 138, 'resembling': 139, 'all': 140, 'where:': 141, \"feel'st\": 142, 'when': 143, 'thereby': 144, 'despite': 145, 'one': 146, 'out-going': 147, \"lov'st\": 148, 'heavenly': 149, 'now': 150, 'were': 151, 'mother': 152, 'they': 153, 'buriest': 154, 'prove': 155, 'fear': 156, 'self-willed': 157, 'pleasing': 158, 'music': 159, \"world's\": 160, 'usury': 161, 'thou': 162, 'same': 163, 'never': 164, 'conquest': 165, 'dig': 166, 'be': 167, 'viewest': 168, \"that's\": 169, 'unbless': 170, 'wrinkles': 171, 'show': 172, 'niggarding:': 173, 'ordering;': 174, 'must': 175, 'willing': 176, \"beauty's\": 177, 'weed': 178, \"hear'st\": 179, 'fairly': 180, 'as': 181, 'could': 182, 'bareness': 183, 'offend': 184, 'lives': 185, 'mine': 186, 'dies': 187, 'riper': 188, 'so': 189, 'living': 190, 'content': 191, 'distillation': 192, 'unused': 193, \"none'\": 194, 'excel:': 195, 'tomb': 196, 'asked': 197, 'happy': 198, 'remembered': 199, 'gazed': 200, 'singleness': 201, 'use': 202, 'new-appearing': 203, 'strong': 204, 'wet': 205, 'within': 206, 'forty': 207, 'sums': 208, 'some': 209, 'gaudy': 210, 'pent': 211, 'winters': 212, 'executor': 213, 'still': 214, 'sap': 215, 'mutual': 216, 'bright': 217, \"feed'st\": 218, 'thriftless': 219, 'couldst': 220, 'live': 221, 'death': 222, 'do': 223, 'looks': 224, 'gentle': 225, 'not': 226, 'field': 227, 'bear:': 228, 'rose': 229, 'herald': 230, 'golden': 231, 'own': 232, 'unless': 233, 'light': 234, 'deface': 235, 'too': 236, 'will': 237, 'self-love': 238, 'largess': 239, 'memory:': 240, 'self-killed:': 241, 'who': 242, 'calls': 243, 'lend': 244, 'let': 245, 'sadly': 246, 'sounds': 247, 'unthrifty': 248, 'face': 249, 'worth': 250, 'winter': 251, 'those': 252, 'meet': 253, 'prisoner': 254, 'look': 255, 'there': 256, 'remembrance': 257, 'nature': 258, 'from': 259, 'of': 260, 'acceptable': 261, 'this': 262, 'sweets': 263, 'grave': 264, 'beguile': 265, \"widow's\": 266, 'bud': 267, 'deserved': 268, 'doth': 269, 'ere': 270, 'if': 271, 'sonnets': 272, 'unions': 273, 'another': 274, 'sweetly': 275, 'ragged': 276, 'self-substantial': 277, 'famine': 278, 'quite': 279, 'sacred': 280, 'diest': 281, 'war': 282, 'distilled': 283, 'pilgrimage:': 284, 'shall': 285, 'flowers': 286, 'to': 287, \"'this\": 288, 'treasure': 289, 'much': 290, 'hill': 291, 'frame': 292, 'art': 293, 'used': 294, 'cold': 295, 'dost': 296, 'joy:': 297, \"mother's\": 298, 'whose': 299, 'for': 300, 'fore': 301, 'forbidden': 302, 'joy': 303, 'age': 304, 'small': 305, 'why': 306, 'more': 307, 'loveliness': 308, \"youth's\": 309, 'heir': 310, 'shalt': 311, 'yet': 312, 'might': 313, 'contracted': 314, 'hear': 315, 'we': 316, 'thee': 317, 'thee:': 318, 'creatures': 319, 'spring': 320, 'warm': 321, 'liquid': 322, 'breed': 323, 'get': 324, 'true': 325, 'frank': 326, 'well-tuned': 327, 'traffic': 328, 'bounteous': 329, 'tender': 330, 'attending': 331, 'fuel': 332, 'thine': 333, 'happies': 334, 'gives': 335, 'else': 336, 'pleasure': 337, 'lo': 338, 'given': 339, 'bereft': 340, 'desire': 341, 'unlooked': 342, 'leaves': 343, 'having': 344, 'orient': 345, 'profitless': 346, 'duteous': 347, 'every': 348, 'steep-up': 349, 'being': 350, 'day': 351, 'car': 352, 'deceive': 353, 'great': 354, \"o'er-snowed\": 355, 'uneared': 356, 'was': 357, 'abundance': 358, 'what': 359, 'increase': 360, 'concord': 361, 'foe': 362, 'glutton': 363, 'repair': 364, 'william': 365, 'leave': 366, 'canst': 367, 'parts': 368, 'checked': 369, 'should': 370, 'nor': 371, 'nothing': 372, 'is': 373, 'say': 374, 'noon:': 375, 'sings': 376, 'decease': 377, 'husband': 378, 'sire': 379, 'alone': 380, 'unfair': 381, \"receiv'st\": 382, 'audit': 383, 'brow': 384, 'fresh': 385, 'succession': 386, 'tell': 387, 'a': 388, 'lifts': 389, 'thy': 390, 'on': 391, 'besiege': 392, 'her': 393, 'shouldst': 394, 'adore': 395, 'which': 396, 'their': 397, \"'thou\": 398, 'legacy': 399, 'old': 400, 'leads': 401, 'dwell': 402, 'disdains': 403, 'under': 404, 'seeming': 405, 'happier': 406, 'single': 407, 'eat': 408, 'pitch': 409, 'wilt': 410, 'then': 411, 'walls': 412, 'sweet': 413, 'chide': 414, 'proving': 415, 'free:': 416, 'leaving': 417, 'distilled:': 418, 'substance': 419, 'blood': 420, 'tillage': 421, 'speechless': 422, 'shakespeare': 423, 'sight': 424, 'cruel:': 425, 'he': 426, 'him': 427, 'with': 428, 'low': 429, 'count': 430, 'give': 431, 'livery': 432, 'many': 433, 'hand': 434, 'tombed': 435, 'effect': 436, 'tract': 437, 'glass': 438, 'summer': 439, 'mark': 440, 'string': 441}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGtag7LptYBm",
        "outputId": "9272c39d-ebf7-4c80-ad2a-40eb3a6a7b5d"
      },
      "source": [
        "len(data) // 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15jXe7jiQQyS"
      },
      "source": [
        "seqlen = 20\n",
        "num_batches = 44\n",
        "\n",
        "data_trimmed = data[:num_batches * seqlen]\n",
        "batches = []\n",
        "for j in range(0,len(data_trimmed) - seqlen, seqlen):\n",
        "  batch = []\n",
        "  x_batch = [ch2id[ch] for ch in data_trimmedj+1:j+seqlen+1]]\n",
        "  batch.append(x_batch)\n",
        "  batch.append(y_batch)\n",
        "  batches.append(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAFnMHY0UWvF",
        "outputId": "e578dd88-da19-43fa-f34d-5f6dae737ebb"
      },
      "source": [
        "len(batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVUC96wsYxJ4"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "  x_safe = x + 1e-12\n",
        "  f = 1 / (1 + np.exp(-x_safe))\n",
        "  \n",
        "  if derivative: # Return the derivative of the function evaluated at x\n",
        "      return f * (1 - f)\n",
        "  else: # Return the forward pass of the function at x\n",
        "      return f\n",
        "\n",
        "def tanh(x, derivative=False):\n",
        "  x_safe = x + 1e-12\n",
        "  f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
        "  \n",
        "  if derivative: # Return the derivative of the function evaluated at x\n",
        "      return 1-f**2\n",
        "  else: # Return the forward pass of the function at x\n",
        "      return f\n",
        "\n",
        "def softmax(x, derivative=False):\n",
        "  x_safe = x + 1e-12\n",
        "  f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
        "  \n",
        "  if derivative: # Return the derivative of the function evaluated at x\n",
        "      pass # We will not need this one\n",
        "  else: # Return the forward pass of the function at x\n",
        "      return f\n",
        "\n",
        "# def softmax(x):\n",
        "#   e_x = np.exp(x - np.max(x))\n",
        "#   return e_x / e_x.sum(axis=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46vDc7p5ZDcH"
      },
      "source": [
        "# Implementing the LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Ha8KmT37pM"
      },
      "source": [
        "# def init_orthogonal(param):\n",
        "#   if param.ndim < 2:\n",
        "#         raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
        "\n",
        "#   rows, cols = param.shape\n",
        "  \n",
        "#   new_param = np.random.randn(rows, cols)\n",
        "  \n",
        "#   if rows < cols:\n",
        "#       new_param = new_param.T\n",
        "#   q, r = np.linalg.qr(new_param)\n",
        "  \n",
        "\n",
        "#   d = np.diag(r, 0)\n",
        "#   ph = np.sign(d)\n",
        "#   q *= ph\n",
        "\n",
        "#   if rows < cols:\n",
        "#       q = q.T\n",
        "  \n",
        "#   new_param = q\n",
        "    \n",
        "#   return new_param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZl9SSXDZ9gd"
      },
      "source": [
        "hidden_size = 64\n",
        "\n",
        "z_size = hidden_size + vocab_size\n",
        "\n",
        "def init_lstm(hidden_size, vocab_size, z_size):\n",
        "  W_f = np.zeros((hidden_size, hidden_size+vocab_size))\n",
        "  b_f = np.zeros((hidden_size, 1))\n",
        "  W_i = np.zeros((hidden_size, hidden_size+vocab_size))\n",
        "  b_i = np.zeros((hidden_size,1))\n",
        "  W_g = np.zeros((hidden_size, hidden_size+vocab_size))\n",
        "  b_g = np.zeros((hidden_size, 1))\n",
        "  W_o = np.zeros((hidden_size, hidden_size+vocab_size))\n",
        "  b_o = np.zeros((hidden_size, 1))\n",
        "  W_v = np.zeros((vocab_size,hidden_size ))\n",
        "  b_v = np.zeros((vocab_size, 1))\n",
        "\n",
        "  W_f = init_orthogonal(W_f)\n",
        "  W_i = init_orthogonal(W_i)\n",
        "  W_g = init_orthogonal(W_g)\n",
        "  W_o = init_orthogonal(W_o)\n",
        "  W_v = init_orthogonal(W_v)\n",
        "\n",
        "  return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
        "\n",
        "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCs-CJ7fyS-5"
      },
      "source": [
        "def update_parameters(params, grads, lr=1e-3):\n",
        "  # Take a step\n",
        "  for param, grad in zip(params, grads):\n",
        "      param -= (lr * grad)\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhhZqZrxwLKX"
      },
      "source": [
        "def clip_gradient_norm(grads, max_norm=0.25):\n",
        "  max_norm = float(max_norm)\n",
        "  total_norm = 0\n",
        "  \n",
        "  # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
        "  for grad in grads:\n",
        "      grad_norm = np.sum(np.power(grad, 2))\n",
        "      total_norm += grad_norm\n",
        "  \n",
        "  total_norm = np.sqrt(total_norm)\n",
        "  \n",
        "  # Calculate clipping coeficient\n",
        "  clip_coef = max_norm / (total_norm + 1e-6)\n",
        "  \n",
        "  # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
        "  if clip_coef < 1:\n",
        "      for grad in grads:\n",
        "          grad *= clip_coef\n",
        "  \n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1egrBKsL4Yzj"
      },
      "source": [
        "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
        "\n",
        "def forward(inputs, h_prev, C_prev, p):\n",
        "  assert h_prev.shape == (hidden_size, 1)\n",
        "  assert C_prev.shape == (hidden_size, 1)\n",
        "\n",
        "  # First we unpack our parameters\n",
        "  W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
        "  \n",
        "  # Save a list of computations for each of the components in the LSTM\n",
        "  x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
        "  g_s, C_s, o_s, h_s = [], [] ,[], []\n",
        "  v_s, outputs =  [], [] \n",
        "  \n",
        "  # Append the initial cell and hidden state to their respective lists\n",
        "  h_s.append(h_prev)\n",
        "  C_s.append(C_prev)\n",
        "\n",
        "  for x in inputs:\n",
        "    z = np.row_stack((h_prev,x))\n",
        "    z_s.append(z)\n",
        "\n",
        "    f = sigmoid(np.dot(W_f, z) + b_f)\n",
        "    f_s.append(f)\n",
        "\n",
        "    i = sigmoid(np.dot(W_i,z) + b_i)\n",
        "    i_s.append(i)\n",
        "\n",
        "    g = tanh(np.dot(W_g, z) + b_g)\n",
        "    g_s.append(g)\n",
        "\n",
        "    C_prev = C_prev * f + g * i\n",
        "    C_s.append(C_prev)\n",
        "\n",
        "    o = sigmoid(np.dot(W_o,z) + b_o)\n",
        "    o_s.append(o)\n",
        "\n",
        "    h_prev = o * tanh(C_prev)\n",
        "    h_s.append(h_prev)\n",
        "\n",
        "    # Calculate logits\n",
        "    v = np.dot(W_v, h_prev) + b_v\n",
        "    v_s.append(v)\n",
        "    \n",
        "    # Calculate softmax\n",
        "    output = softmax(v)\n",
        "    outputs.append(output)\n",
        "\n",
        "  return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsbuAvhB5atg"
      },
      "source": [
        "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
        "\n",
        "    # Unpack parameters\n",
        "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
        "\n",
        "    # Initialize gradients as zero\n",
        "    W_f_d = np.zeros_like(W_f)\n",
        "    b_f_d = np.zeros_like(b_f)\n",
        "\n",
        "    W_i_d = np.zeros_like(W_i)\n",
        "    b_i_d = np.zeros_like(b_i)\n",
        "\n",
        "    W_g_d = np.zeros_like(W_g)\n",
        "    b_g_d = np.zeros_like(b_g)\n",
        "\n",
        "    W_o_d = np.zeros_like(W_o)\n",
        "    b_o_d = np.zeros_like(b_o)\n",
        "\n",
        "    W_v_d = np.zeros_like(W_v)\n",
        "    b_v_d = np.zeros_like(b_v)\n",
        "    \n",
        "    # Set the next cell and hidden state equal to zero\n",
        "    dh_next = np.zeros_like(h[0])\n",
        "    dC_next = np.zeros_like(C[0])\n",
        "        \n",
        "    # Track loss\n",
        "    loss = 0\n",
        "    \n",
        "    for t in reversed(range(len(outputs))):\n",
        "        \n",
        "        # Compute the cross entropy\n",
        "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
        "        # Get the previous hidden cell state\n",
        "        C_prev= C[t-1]\n",
        "        \n",
        "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
        "        dv = np.copy(outputs[t])\n",
        "        dv[np.argmax(targets[t])] -= 1\n",
        "\n",
        "        # Update the gradient of the relation of the hidden-state to the output gate\n",
        "        W_v_d += np.dot(dv, h[t].T)\n",
        "        b_v_d += dv\n",
        "\n",
        "        # Compute the derivative of the hidden state and output gate\n",
        "        dh = np.dot(W_v.T, dv)        \n",
        "        dh += dh_next\n",
        "        do = dh * tanh(C[t])\n",
        "        do = sigmoid(o[t], derivative=True)*do\n",
        "        \n",
        "        # Update the gradients with respect to the output gate\n",
        "        W_o_d += np.dot(do, z[t].T)\n",
        "        b_o_d += do\n",
        "\n",
        "        # Compute the derivative of the cell state and candidate g\n",
        "        dC = np.copy(dC_next)\n",
        "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
        "        dg = dC * i[t]\n",
        "        dg = tanh(g[t], derivative=True) * dg\n",
        "        \n",
        "        # Update the gradients with respect to the candidate\n",
        "        W_g_d += np.dot(dg, z[t].T)\n",
        "        b_g_d += dg\n",
        "\n",
        "        # Compute the derivative of the input gate and update its gradients\n",
        "        di = dC * g[t]\n",
        "        di = sigmoid(i[t], True) * di\n",
        "        W_i_d += np.dot(di, z[t].T)\n",
        "        b_i_d += di\n",
        "\n",
        "        # Compute the derivative of the forget gate and update its gradients\n",
        "        df = dC * C_prev\n",
        "        df = sigmoid(f[t]) * df\n",
        "        W_f_d += np.dot(df, z[t].T)\n",
        "        b_f_d += df\n",
        "\n",
        "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
        "        dz = (np.dot(W_f.T, df)\n",
        "             + np.dot(W_i.T, di)\n",
        "             + np.dot(W_g.T, dg)\n",
        "             + np.dot(W_o.T, do))\n",
        "        dh_prev = dz[:hidden_size, :]\n",
        "        dC_prev = f[t] * dC\n",
        "        \n",
        "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
        "    \n",
        "    # Clip gradients\n",
        "    grads = clip_gradient_norm(grads)\n",
        "    \n",
        "    return loss, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSbFJeaOyOHy"
      },
      "source": [
        "def one_hot_encode(idx,vocab_size):\n",
        "  one_hot = np.zeros((vocab_size,1))\n",
        "  one_hot[idx] = 1.0\n",
        "  return one_hot\n",
        "\n",
        "def one_hot_sequence(sequence, vocab_size):\n",
        "  # here sequence is numbers of ids\n",
        "  encoding = np.array([one_hot_encode(word,vocab_size) for word in sequence])\n",
        "  encoding = encoding.reshape(encoding.shape[0], encoding.shape[1],1)\n",
        "  return encoding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th-GIQBxatMa",
        "outputId": "15443193-7b8a-4699-f941-7180ca3f802d"
      },
      "source": [
        "num_epochs = 1000\n",
        "z_size = hidden_size + vocab_size\n",
        "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
        "\n",
        "hidden_state = np.zeros((hidden_size,1))\n",
        "training_loss= []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  epoch_training_loss = 0\n",
        "  h = np.zeros((hidden_size,1))\n",
        "  c = np.zeros((hidden_size,1))  \n",
        "  for batch in batches:\n",
        "    x_batch = batch[0]\n",
        "    y_batch = batch[1]\n",
        "    \n",
        "    x_seq = one_hot_sequence(x_batch, vocab_size)\n",
        "    y_seq = one_hot_sequence(y_batch, vocab_size)\n",
        "\n",
        "\n",
        "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(x_seq, h, c, params)\n",
        "    loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, y_seq, params)\n",
        "    # print(\"Params before : \" ,params[0])\n",
        "    params = update_parameters(params = params, grads = grads, lr = 0.001)\n",
        "    # print(\"Params after : \", params[0])\n",
        "  epoch_training_loss += loss\n",
        "  training_loss.append(epoch_training_loss)\n",
        "  print(f'Epoch {i}, training loss: {training_loss[-1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training loss: 0.27556917303528033\n",
            "Epoch 1, training loss: 0.27555932547069745\n",
            "Epoch 2, training loss: 0.2755494799730793\n",
            "Epoch 3, training loss: 0.27553963654323116\n",
            "Epoch 4, training loss: 0.2755297951819593\n",
            "Epoch 5, training loss: 0.275519955890071\n",
            "Epoch 6, training loss: 0.2755101186683745\n",
            "Epoch 7, training loss: 0.275500283517679\n",
            "Epoch 8, training loss: 0.2754904504387949\n",
            "Epoch 9, training loss: 0.27548061943253316\n",
            "Epoch 10, training loss: 0.2754707904997061\n",
            "Epoch 11, training loss: 0.275460963641127\n",
            "Epoch 12, training loss: 0.27545113885760986\n",
            "Epoch 13, training loss: 0.27544131614997\n",
            "Epoch 14, training loss: 0.2754314955190233\n",
            "Epoch 15, training loss: 0.275421676965587\n",
            "Epoch 16, training loss: 0.2754118604904791\n",
            "Epoch 17, training loss: 0.2754020460945186\n",
            "Epoch 18, training loss: 0.27539223377852545\n",
            "Epoch 19, training loss: 0.27538242354332054\n",
            "Epoch 20, training loss: 0.2753726153897258\n",
            "Epoch 21, training loss: 0.275362809318564\n",
            "Epoch 22, training loss: 0.2753530053306589\n",
            "Epoch 23, training loss: 0.27534320342683516\n",
            "Epoch 24, training loss: 0.27533340360791836\n",
            "Epoch 25, training loss: 0.27532360587473537\n",
            "Epoch 26, training loss: 0.27531381022811346\n",
            "Epoch 27, training loss: 0.2753040166688811\n",
            "Epoch 28, training loss: 0.27529422519786767\n",
            "Epoch 29, training loss: 0.2752844358159036\n",
            "Epoch 30, training loss: 0.27527464852381994\n",
            "Epoch 31, training loss: 0.2752648633224489\n",
            "Epoch 32, training loss: 0.2752550802126235\n",
            "Epoch 33, training loss: 0.2752452991951778\n",
            "Epoch 34, training loss: 0.27523552027094655\n",
            "Epoch 35, training loss: 0.2752257434407656\n",
            "Epoch 36, training loss: 0.2752159687054717\n",
            "Epoch 37, training loss: 0.2752061960659023\n",
            "Epoch 38, training loss: 0.27519642552289586\n",
            "Epoch 39, training loss: 0.2751866570772919\n",
            "Epoch 40, training loss: 0.2751768907299307\n",
            "Epoch 41, training loss: 0.2751671264816532\n",
            "Epoch 42, training loss: 0.2751573643333017\n",
            "Epoch 43, training loss: 0.275147604285719\n",
            "Epoch 44, training loss: 0.27513784633974886\n",
            "Epoch 45, training loss: 0.275128090496236\n",
            "Epoch 46, training loss: 0.27511833675602604\n",
            "Epoch 47, training loss: 0.2751085851199653\n",
            "Epoch 48, training loss: 0.2750988355889011\n",
            "Epoch 49, training loss: 0.2750890881636816\n",
            "Epoch 50, training loss: 0.275079342845156\n",
            "Epoch 51, training loss: 0.2750695996341739\n",
            "Epoch 52, training loss: 0.27505985853158604\n",
            "Epoch 53, training loss: 0.27505011953824426\n",
            "Epoch 54, training loss: 0.2750403826550007\n",
            "Epoch 55, training loss: 0.27503064788270876\n",
            "Epoch 56, training loss: 0.27502091522222266\n",
            "Epoch 57, training loss: 0.2750111846743972\n",
            "Epoch 58, training loss: 0.2750014562400883\n",
            "Epoch 59, training loss: 0.2749917299201524\n",
            "Epoch 60, training loss: 0.27498200571544723\n",
            "Epoch 61, training loss: 0.27497228362683096\n",
            "Epoch 62, training loss: 0.27496256365516264\n",
            "Epoch 63, training loss: 0.27495284580130236\n",
            "Epoch 64, training loss: 0.2749431300661106\n",
            "Epoch 65, training loss: 0.27493341645044933\n",
            "Epoch 66, training loss: 0.27492370495518065\n",
            "Epoch 67, training loss: 0.27491399558116797\n",
            "Epoch 68, training loss: 0.27490428832927494\n",
            "Epoch 69, training loss: 0.27489458320036686\n",
            "Epoch 70, training loss: 0.27488488019530904\n",
            "Epoch 71, training loss: 0.2748751793149679\n",
            "Epoch 72, training loss: 0.2748654805602107\n",
            "Epoch 73, training loss: 0.27485578393190546\n",
            "Epoch 74, training loss: 0.27484608943092104\n",
            "Epoch 75, training loss: 0.27483639705812685\n",
            "Epoch 76, training loss: 0.27482670681439336\n",
            "Epoch 77, training loss: 0.27481701870059183\n",
            "Epoch 78, training loss: 0.2748073327175939\n",
            "Epoch 79, training loss: 0.27479764886627245\n",
            "Epoch 80, training loss: 0.27478796714750103\n",
            "Epoch 81, training loss: 0.2747782875621537\n",
            "Epoch 82, training loss: 0.27476861011110565\n",
            "Epoch 83, training loss: 0.2747589347952325\n",
            "Epoch 84, training loss: 0.27474926161541086\n",
            "Epoch 85, training loss: 0.27473959057251796\n",
            "Epoch 86, training loss: 0.2747299216674319\n",
            "Epoch 87, training loss: 0.27472025490103147\n",
            "Epoch 88, training loss: 0.2747105902741962\n",
            "Epoch 89, training loss: 0.2747009277878064\n",
            "Epoch 90, training loss: 0.27469126744274325\n",
            "Epoch 91, training loss: 0.2746816092398881\n",
            "Epoch 92, training loss: 0.274671953180124\n",
            "Epoch 93, training loss: 0.2746622992643338\n",
            "Epoch 94, training loss: 0.2746526474934016\n",
            "Epoch 95, training loss: 0.2746429978682123\n",
            "Epoch 96, training loss: 0.274633350389651\n",
            "Epoch 97, training loss: 0.2746237050586041\n",
            "Epoch 98, training loss: 0.27461406187595844\n",
            "Epoch 99, training loss: 0.27460442084260156\n",
            "Epoch 100, training loss: 0.2745947819594218\n",
            "Epoch 101, training loss: 0.2745851452273082\n",
            "Epoch 102, training loss: 0.27457551064715036\n",
            "Epoch 103, training loss: 0.27456587821983885\n",
            "Epoch 104, training loss: 0.2745562479462649\n",
            "Epoch 105, training loss: 0.2745466198273201\n",
            "Epoch 106, training loss: 0.2745369938638972\n",
            "Epoch 107, training loss: 0.2745273700568893\n",
            "Epoch 108, training loss: 0.27451774840719045\n",
            "Epoch 109, training loss: 0.27450812891569504\n",
            "Epoch 110, training loss: 0.27449851158329863\n",
            "Epoch 111, training loss: 0.274488896410897\n",
            "Epoch 112, training loss: 0.27447928339938693\n",
            "Epoch 113, training loss: 0.2744696725496657\n",
            "Epoch 114, training loss: 0.2744600638626314\n",
            "Epoch 115, training loss: 0.27445045733918244\n",
            "Epoch 116, training loss: 0.27444085298021853\n",
            "Epoch 117, training loss: 0.2744312507866395\n",
            "Epoch 118, training loss: 0.27442165075934594\n",
            "Epoch 119, training loss: 0.2744120528992394\n",
            "Epoch 120, training loss: 0.2744024572072218\n",
            "Epoch 121, training loss: 0.2743928636841956\n",
            "Epoch 122, training loss: 0.2743832723310643\n",
            "Epoch 123, training loss: 0.27437368314873184\n",
            "Epoch 124, training loss: 0.27436409613810275\n",
            "Epoch 125, training loss: 0.2743545113000823\n",
            "Epoch 126, training loss: 0.2743449286355764\n",
            "Epoch 127, training loss: 0.27433534814549143\n",
            "Epoch 128, training loss: 0.2743257698307347\n",
            "Epoch 129, training loss: 0.2743161936922139\n",
            "Epoch 130, training loss: 0.2743066197308374\n",
            "Epoch 131, training loss: 0.27429704794751436\n",
            "Epoch 132, training loss: 0.2742874783431543\n",
            "Epoch 133, training loss: 0.2742779109186675\n",
            "Epoch 134, training loss: 0.274268345674965\n",
            "Epoch 135, training loss: 0.27425878261295816\n",
            "Epoch 136, training loss: 0.27424922173355915\n",
            "Epoch 137, training loss: 0.2742396630376807\n",
            "Epoch 138, training loss: 0.2742301065262362\n",
            "Epoch 139, training loss: 0.2742205522001394\n",
            "Epoch 140, training loss: 0.274211000060305\n",
            "Epoch 141, training loss: 0.2742014501076481\n",
            "Epoch 142, training loss: 0.2741919023430845\n",
            "Epoch 143, training loss: 0.27418235676753033\n",
            "Epoch 144, training loss: 0.2741728133819026\n",
            "Epoch 145, training loss: 0.2741632721871189\n",
            "Epoch 146, training loss: 0.2741537331840972\n",
            "Epoch 147, training loss: 0.2741441963737562\n",
            "Epoch 148, training loss: 0.27413466175701506\n",
            "Epoch 149, training loss: 0.27412512933479355\n",
            "Epoch 150, training loss: 0.27411559910801225\n",
            "Epoch 151, training loss: 0.27410607107759205\n",
            "Epoch 152, training loss: 0.27409654524445437\n",
            "Epoch 153, training loss: 0.27408702160952136\n",
            "Epoch 154, training loss: 0.2740775001737157\n",
            "Epoch 155, training loss: 0.2740679809379605\n",
            "Epoch 156, training loss: 0.2740584639031795\n",
            "Epoch 157, training loss: 0.2740489490702972\n",
            "Epoch 158, training loss: 0.2740394364402382\n",
            "Epoch 159, training loss: 0.2740299260139281\n",
            "Epoch 160, training loss: 0.2740204177922928\n",
            "Epoch 161, training loss: 0.27401091177625875\n",
            "Epoch 162, training loss: 0.274001407966753\n",
            "Epoch 163, training loss: 0.27399190636470316\n",
            "Epoch 164, training loss: 0.27398240697103715\n",
            "Epoch 165, training loss: 0.2739729097866838\n",
            "Epoch 166, training loss: 0.27396341481257225\n",
            "Epoch 167, training loss: 0.27395392204963204\n",
            "Epoch 168, training loss: 0.27394443149879344\n",
            "Epoch 169, training loss: 0.2739349431609872\n",
            "Epoch 170, training loss: 0.2739254570371446\n",
            "Epoch 171, training loss: 0.27391597312819727\n",
            "Epoch 172, training loss: 0.2739064914350776\n",
            "Epoch 173, training loss: 0.27389701195871813\n",
            "Epoch 174, training loss: 0.27388753470005245\n",
            "Epoch 175, training loss: 0.2738780596600141\n",
            "Epoch 176, training loss: 0.2738685868395374\n",
            "Epoch 177, training loss: 0.27385911623955717\n",
            "Epoch 178, training loss: 0.27384964786100857\n",
            "Epoch 179, training loss: 0.2738401817048275\n",
            "Epoch 180, training loss: 0.27383071777195017\n",
            "Epoch 181, training loss: 0.2738212560633132\n",
            "Epoch 182, training loss: 0.27381179657985383\n",
            "Epoch 183, training loss: 0.27380233932250997\n",
            "Epoch 184, training loss: 0.27379288429221943\n",
            "Epoch 185, training loss: 0.2737834314899211\n",
            "Epoch 186, training loss: 0.27377398091655414\n",
            "Epoch 187, training loss: 0.27376453257305794\n",
            "Epoch 188, training loss: 0.27375508646037267\n",
            "Epoch 189, training loss: 0.2737456425794388\n",
            "Epoch 190, training loss: 0.27373620093119727\n",
            "Epoch 191, training loss: 0.27372676151658965\n",
            "Epoch 192, training loss: 0.27371732433655765\n",
            "Epoch 193, training loss: 0.2737078893920437\n",
            "Epoch 194, training loss: 0.2736984566839907\n",
            "Epoch 195, training loss: 0.27368902621334174\n",
            "Epoch 196, training loss: 0.2736795979810404\n",
            "Epoch 197, training loss: 0.273670171988031\n",
            "Epoch 198, training loss: 0.273660748235258\n",
            "Epoch 199, training loss: 0.2736513267236666\n",
            "Epoch 200, training loss: 0.2736419074542019\n",
            "Epoch 201, training loss: 0.2736324904278099\n",
            "Epoch 202, training loss: 0.273623075645437\n",
            "Epoch 203, training loss: 0.27361366310802976\n",
            "Epoch 204, training loss: 0.27360425281653533\n",
            "Epoch 205, training loss: 0.2735948447719013\n",
            "Epoch 206, training loss: 0.27358543897507576\n",
            "Epoch 207, training loss: 0.2735760354270068\n",
            "Epoch 208, training loss: 0.27356663412864346\n",
            "Epoch 209, training loss: 0.2735572350809349\n",
            "Epoch 210, training loss: 0.27354783828483065\n",
            "Epoch 211, training loss: 0.2735384437412807\n",
            "Epoch 212, training loss: 0.2735290514512356\n",
            "Epoch 213, training loss: 0.27351966141564604\n",
            "Epoch 214, training loss: 0.2735102736354634\n",
            "Epoch 215, training loss: 0.273500888111639\n",
            "Epoch 216, training loss: 0.2734915048451249\n",
            "Epoch 217, training loss: 0.27348212383687365\n",
            "Epoch 218, training loss: 0.27347274508783787\n",
            "Epoch 219, training loss: 0.27346336859897064\n",
            "Epoch 220, training loss: 0.27345399437122564\n",
            "Epoch 221, training loss: 0.27344462240555656\n",
            "Epoch 222, training loss: 0.2734352527029178\n",
            "Epoch 223, training loss: 0.2734258852642639\n",
            "Epoch 224, training loss: 0.27341652009054995\n",
            "Epoch 225, training loss: 0.27340715718273123\n",
            "Epoch 226, training loss: 0.27339779654176344\n",
            "Epoch 227, training loss: 0.2733884381686028\n",
            "Epoch 228, training loss: 0.27337908206420564\n",
            "Epoch 229, training loss: 0.2733697282295287\n",
            "Epoch 230, training loss: 0.27336037666552937\n",
            "Epoch 231, training loss: 0.2733510273731648\n",
            "Epoch 232, training loss: 0.27334168035339307\n",
            "Epoch 233, training loss: 0.27333233560717224\n",
            "Epoch 234, training loss: 0.2733229931354609\n",
            "Epoch 235, training loss: 0.2733136529392179\n",
            "Epoch 236, training loss: 0.2733043150194022\n",
            "Epoch 237, training loss: 0.2732949793769738\n",
            "Epoch 238, training loss: 0.2732856460128922\n",
            "Epoch 239, training loss: 0.2732763149281176\n",
            "Epoch 240, training loss: 0.2732669861236105\n",
            "Epoch 241, training loss: 0.2732576596003318\n",
            "Epoch 242, training loss: 0.2732483353592427\n",
            "Epoch 243, training loss: 0.2732390134013044\n",
            "Epoch 244, training loss: 0.2732296937274789\n",
            "Epoch 245, training loss: 0.2732203763387283\n",
            "Epoch 246, training loss: 0.27321106123601474\n",
            "Epoch 247, training loss: 0.2732017484203011\n",
            "Epoch 248, training loss: 0.27319243789255043\n",
            "Epoch 249, training loss: 0.2731831296537258\n",
            "Epoch 250, training loss: 0.273173823704791\n",
            "Epoch 251, training loss: 0.27316452004670966\n",
            "Epoch 252, training loss: 0.2731552186804461\n",
            "Epoch 253, training loss: 0.27314591960696494\n",
            "Epoch 254, training loss: 0.27313662282723067\n",
            "Epoch 255, training loss: 0.27312732834220843\n",
            "Epoch 256, training loss: 0.27311803615286345\n",
            "Epoch 257, training loss: 0.2731087462601614\n",
            "Epoch 258, training loss: 0.27309945866506813\n",
            "Epoch 259, training loss: 0.27309017336854974\n",
            "Epoch 260, training loss: 0.27308089037157257\n",
            "Epoch 261, training loss: 0.27307160967510336\n",
            "Epoch 262, training loss: 0.27306233128010904\n",
            "Epoch 263, training loss: 0.27305305518755674\n",
            "Epoch 264, training loss: 0.2730437813984139\n",
            "Epoch 265, training loss: 0.27303450991364836\n",
            "Epoch 266, training loss: 0.273025240734228\n",
            "Epoch 267, training loss: 0.2730159738611209\n",
            "Epoch 268, training loss: 0.2730067092952957\n",
            "Epoch 269, training loss: 0.272997447037721\n",
            "Epoch 270, training loss: 0.2729881870893657\n",
            "Epoch 271, training loss: 0.27297892945119906\n",
            "Epoch 272, training loss: 0.2729696741241905\n",
            "Epoch 273, training loss: 0.27296042110930957\n",
            "Epoch 274, training loss: 0.2729511704075263\n",
            "Epoch 275, training loss: 0.27294192201981066\n",
            "Epoch 276, training loss: 0.2729326759471331\n",
            "Epoch 277, training loss: 0.272923432190464\n",
            "Epoch 278, training loss: 0.27291419075077433\n",
            "Epoch 279, training loss: 0.272904951629035\n",
            "Epoch 280, training loss: 0.2728957148262173\n",
            "Epoch 281, training loss: 0.2728864803432925\n",
            "Epoch 282, training loss: 0.27287724818123227\n",
            "Epoch 283, training loss: 0.27286801834100854\n",
            "Epoch 284, training loss: 0.2728587908235933\n",
            "Epoch 285, training loss: 0.2728495656299589\n",
            "Epoch 286, training loss: 0.27284034276107766\n",
            "Epoch 287, training loss: 0.2728311222179221\n",
            "Epoch 288, training loss: 0.2728219040014653\n",
            "Epoch 289, training loss: 0.2728126881126802\n",
            "Epoch 290, training loss: 0.2728034745525399\n",
            "Epoch 291, training loss: 0.27279426332201795\n",
            "Epoch 292, training loss: 0.27278505442208784\n",
            "Epoch 293, training loss: 0.27277584785372333\n",
            "Epoch 294, training loss: 0.27276664361789843\n",
            "Epoch 295, training loss: 0.2727574417155872\n",
            "Epoch 296, training loss: 0.27274824214776394\n",
            "Epoch 297, training loss: 0.2727390449154029\n",
            "Epoch 298, training loss: 0.2727298500194791\n",
            "Epoch 299, training loss: 0.27272065746096696\n",
            "Epoch 300, training loss: 0.27271146724084155\n",
            "Epoch 301, training loss: 0.27270227936007807\n",
            "Epoch 302, training loss: 0.2726930938196517\n",
            "Epoch 303, training loss: 0.2726839106205377\n",
            "Epoch 304, training loss: 0.2726747297637118\n",
            "Epoch 305, training loss: 0.2726655512501497\n",
            "Epoch 306, training loss: 0.2726563750808272\n",
            "Epoch 307, training loss: 0.27264720125672026\n",
            "Epoch 308, training loss: 0.27263802977880514\n",
            "Epoch 309, training loss: 0.27262886064805797\n",
            "Epoch 310, training loss: 0.2726196938654552\n",
            "Epoch 311, training loss: 0.2726105294319734\n",
            "Epoch 312, training loss: 0.2726013673485892\n",
            "Epoch 313, training loss: 0.27259220761627934\n",
            "Epoch 314, training loss: 0.2725830502360208\n",
            "Epoch 315, training loss: 0.27257389520879066\n",
            "Epoch 316, training loss: 0.272564742535566\n",
            "Epoch 317, training loss: 0.27255559221732406\n",
            "Epoch 318, training loss: 0.27254644425504226\n",
            "Epoch 319, training loss: 0.27253729864969806\n",
            "Epoch 320, training loss: 0.27252815540226905\n",
            "Epoch 321, training loss: 0.27251901451373295\n",
            "Epoch 322, training loss: 0.27250987598506754\n",
            "Epoch 323, training loss: 0.27250073981725076\n",
            "Epoch 324, training loss: 0.27249160601126055\n",
            "Epoch 325, training loss: 0.27248247456807506\n",
            "Epoch 326, training loss: 0.2724733454886724\n",
            "Epoch 327, training loss: 0.27246421877403093\n",
            "Epoch 328, training loss: 0.27245509442512894\n",
            "Epoch 329, training loss: 0.2724459724429449\n",
            "Epoch 330, training loss: 0.27243685282845725\n",
            "Epoch 331, training loss: 0.2724277355826447\n",
            "Epoch 332, training loss: 0.27241862070648576\n",
            "Epoch 333, training loss: 0.2724095082009593\n",
            "Epoch 334, training loss: 0.2724003980670441\n",
            "Epoch 335, training loss: 0.27239129030571907\n",
            "Epoch 336, training loss: 0.272382184917963\n",
            "Epoch 337, training loss: 0.2723730819047552\n",
            "Epoch 338, training loss: 0.27236398126707445\n",
            "Epoch 339, training loss: 0.27235488300589994\n",
            "Epoch 340, training loss: 0.27234578712221086\n",
            "Epoch 341, training loss: 0.27233669361698637\n",
            "Epoch 342, training loss: 0.2723276024912059\n",
            "Epoch 343, training loss: 0.27231851374584864\n",
            "Epoch 344, training loss: 0.2723094273818939\n",
            "Epoch 345, training loss: 0.27230034340032117\n",
            "Epoch 346, training loss: 0.27229126180210994\n",
            "Epoch 347, training loss: 0.27228218258823955\n",
            "Epoch 348, training loss: 0.27227310575968955\n",
            "Epoch 349, training loss: 0.2722640313174395\n",
            "Epoch 350, training loss: 0.27225495926246873\n",
            "Epoch 351, training loss: 0.2722458895957573\n",
            "Epoch 352, training loss: 0.27223682231828433\n",
            "Epoch 353, training loss: 0.2722277574310297\n",
            "Epoch 354, training loss: 0.27221869493497297\n",
            "Epoch 355, training loss: 0.2722096348310937\n",
            "Epoch 356, training loss: 0.27220057712037177\n",
            "Epoch 357, training loss: 0.2721915218037867\n",
            "Epoch 358, training loss: 0.2721824688823182\n",
            "Epoch 359, training loss: 0.2721734183569459\n",
            "Epoch 360, training loss: 0.2721643702286495\n",
            "Epoch 361, training loss: 0.2721553244984087\n",
            "Epoch 362, training loss: 0.2721462811672031\n",
            "Epoch 363, training loss: 0.2721372402360123\n",
            "Epoch 364, training loss: 0.27212820170581614\n",
            "Epoch 365, training loss: 0.27211916557759414\n",
            "Epoch 366, training loss: 0.27211013185232585\n",
            "Epoch 367, training loss: 0.2721011005309909\n",
            "Epoch 368, training loss: 0.27209207161456905\n",
            "Epoch 369, training loss: 0.27208304510403963\n",
            "Epoch 370, training loss: 0.27207402100038225\n",
            "Epoch 371, training loss: 0.27206499930457634\n",
            "Epoch 372, training loss: 0.27205598001760145\n",
            "Epoch 373, training loss: 0.27204696314043686\n",
            "Epoch 374, training loss: 0.27203794867406217\n",
            "Epoch 375, training loss: 0.27202893661945654\n",
            "Epoch 376, training loss: 0.27201992697759936\n",
            "Epoch 377, training loss: 0.2720109197494699\n",
            "Epoch 378, training loss: 0.2720019149360473\n",
            "Epoch 379, training loss: 0.2719929125383107\n",
            "Epoch 380, training loss: 0.27198391255723936\n",
            "Epoch 381, training loss: 0.27197491499381216\n",
            "Epoch 382, training loss: 0.27196591984900825\n",
            "Epoch 383, training loss: 0.27195692712380637\n",
            "Epoch 384, training loss: 0.2719479368191854\n",
            "Epoch 385, training loss: 0.2719389489361243\n",
            "Epoch 386, training loss: 0.27192996347560167\n",
            "Epoch 387, training loss: 0.2719209804385962\n",
            "Epoch 388, training loss: 0.27191199982608644\n",
            "Epoch 389, training loss: 0.2719030216390509\n",
            "Epoch 390, training loss: 0.27189404587846794\n",
            "Epoch 391, training loss: 0.27188507254531596\n",
            "Epoch 392, training loss: 0.2718761016405733\n",
            "Epoch 393, training loss: 0.271867133165218\n",
            "Epoch 394, training loss: 0.2718581671202281\n",
            "Epoch 395, training loss: 0.2718492035065816\n",
            "Epoch 396, training loss: 0.27184024232525633\n",
            "Epoch 397, training loss: 0.2718312835772302\n",
            "Epoch 398, training loss: 0.2718223272634808\n",
            "Epoch 399, training loss: 0.2718133733849858\n",
            "Epoch 400, training loss: 0.2718044219427224\n",
            "Epoch 401, training loss: 0.2717954729376682\n",
            "Epoch 402, training loss: 0.2717865263708004\n",
            "Epoch 403, training loss: 0.2717775822430961\n",
            "Epoch 404, training loss: 0.27176864055553224\n",
            "Epoch 405, training loss: 0.2717597013090858\n",
            "Epoch 406, training loss: 0.2717507645047335\n",
            "Epoch 407, training loss: 0.27174183014345193\n",
            "Epoch 408, training loss: 0.27173289822621766\n",
            "Epoch 409, training loss: 0.2717239687540071\n",
            "Epoch 410, training loss: 0.27171504172779654\n",
            "Epoch 411, training loss: 0.27170611714856174\n",
            "Epoch 412, training loss: 0.2716971950172791\n",
            "Epoch 413, training loss: 0.2716882753349242\n",
            "Epoch 414, training loss: 0.2716793581024727\n",
            "Epoch 415, training loss: 0.2716704433209003\n",
            "Epoch 416, training loss: 0.27166153099118223\n",
            "Epoch 417, training loss: 0.2716526211142937\n",
            "Epoch 418, training loss: 0.27164371369120993\n",
            "Epoch 419, training loss: 0.27163480872290585\n",
            "Epoch 420, training loss: 0.2716259062103559\n",
            "Epoch 421, training loss: 0.27161700615453505\n",
            "Epoch 422, training loss: 0.2716081085564175\n",
            "Epoch 423, training loss: 0.27159921341697757\n",
            "Epoch 424, training loss: 0.2715903207371893\n",
            "Epoch 425, training loss: 0.2715814305180267\n",
            "Epoch 426, training loss: 0.27157254276046333\n",
            "Epoch 427, training loss: 0.2715636574654729\n",
            "Epoch 428, training loss: 0.2715547746340287\n",
            "Epoch 429, training loss: 0.27154589426710396\n",
            "Epoch 430, training loss: 0.2715370163656716\n",
            "Epoch 431, training loss: 0.27152814093070454\n",
            "Epoch 432, training loss: 0.27151926796317527\n",
            "Epoch 433, training loss: 0.27151039746405625\n",
            "Epoch 434, training loss: 0.27150152943431977\n",
            "Epoch 435, training loss: 0.2714926638749378\n",
            "Epoch 436, training loss: 0.2714838007868821\n",
            "Epoch 437, training loss: 0.2714749401711244\n",
            "Epoch 438, training loss: 0.271466082028636\n",
            "Epoch 439, training loss: 0.27145722636038805\n",
            "Epoch 440, training loss: 0.27144837316735165\n",
            "Epoch 441, training loss: 0.2714395224504975\n",
            "Epoch 442, training loss: 0.27143067421079614\n",
            "Epoch 443, training loss: 0.27142182844921786\n",
            "Epoch 444, training loss: 0.27141298516673273\n",
            "Epoch 445, training loss: 0.2714041443643107\n",
            "Epoch 446, training loss: 0.27139530604292134\n",
            "Epoch 447, training loss: 0.271386470203534\n",
            "Epoch 448, training loss: 0.271377636847118\n",
            "Epoch 449, training loss: 0.2713688059746422\n",
            "Epoch 450, training loss: 0.2713599775870752\n",
            "Epoch 451, training loss: 0.2713511516853854\n",
            "Epoch 452, training loss: 0.2713423282705413\n",
            "Epoch 453, training loss: 0.2713335073435104\n",
            "Epoch 454, training loss: 0.27132468890526085\n",
            "Epoch 455, training loss: 0.2713158729567598\n",
            "Epoch 456, training loss: 0.2713070594989745\n",
            "Epoch 457, training loss: 0.27129824853287177\n",
            "Epoch 458, training loss: 0.27128944005941835\n",
            "Epoch 459, training loss: 0.27128063407958086\n",
            "Epoch 460, training loss: 0.27127183059432514\n",
            "Epoch 461, training loss: 0.2712630296046171\n",
            "Epoch 462, training loss: 0.27125423111142244\n",
            "Epoch 463, training loss: 0.2712454351157063\n",
            "Epoch 464, training loss: 0.2712366416184339\n",
            "Epoch 465, training loss: 0.2712278506205698\n",
            "Epoch 466, training loss: 0.2712190621230786\n",
            "Epoch 467, training loss: 0.2712102761269245\n",
            "Epoch 468, training loss: 0.2712014926330713\n",
            "Epoch 469, training loss: 0.27119271164248265\n",
            "Epoch 470, training loss: 0.2711839331561219\n",
            "Epoch 471, training loss: 0.27117515717495183\n",
            "Epoch 472, training loss: 0.27116638369993545\n",
            "Epoch 473, training loss: 0.27115761273203504\n",
            "Epoch 474, training loss: 0.2711488442722126\n",
            "Epoch 475, training loss: 0.2711400783214302\n",
            "Epoch 476, training loss: 0.27113131488064895\n",
            "Epoch 477, training loss: 0.27112255395083035\n",
            "Epoch 478, training loss: 0.27111379553293513\n",
            "Epoch 479, training loss: 0.27110503962792387\n",
            "Epoch 480, training loss: 0.27109628623675663\n",
            "Epoch 481, training loss: 0.2710875353603935\n",
            "Epoch 482, training loss: 0.27107878699979393\n",
            "Epoch 483, training loss: 0.2710700411559172\n",
            "Epoch 484, training loss: 0.2710612978297223\n",
            "Epoch 485, training loss: 0.2710525570221677\n",
            "Epoch 486, training loss: 0.27104381873421163\n",
            "Epoch 487, training loss: 0.27103508296681195\n",
            "Epoch 488, training loss: 0.27102634972092643\n",
            "Epoch 489, training loss: 0.27101761899751203\n",
            "Epoch 490, training loss: 0.27100889079752577\n",
            "Epoch 491, training loss: 0.2710001651219241\n",
            "Epoch 492, training loss: 0.27099144197166314\n",
            "Epoch 493, training loss: 0.2709827213476989\n",
            "Epoch 494, training loss: 0.27097400325098653\n",
            "Epoch 495, training loss: 0.2709652876824813\n",
            "Epoch 496, training loss: 0.27095657464313805\n",
            "Epoch 497, training loss: 0.2709478641339108\n",
            "Epoch 498, training loss: 0.2709391561557538\n",
            "Epoch 499, training loss: 0.2709304507096206\n",
            "Epoch 500, training loss: 0.2709217477964645\n",
            "Epoch 501, training loss: 0.27091304741723815\n",
            "Epoch 502, training loss: 0.2709043495728944\n",
            "Epoch 503, training loss: 0.270895654264385\n",
            "Epoch 504, training loss: 0.2708869614926619\n",
            "Epoch 505, training loss: 0.2708782712586763\n",
            "Epoch 506, training loss: 0.27086958356337926\n",
            "Epoch 507, training loss: 0.2708608984077214\n",
            "Epoch 508, training loss: 0.27085221579265256\n",
            "Epoch 509, training loss: 0.27084353571912273\n",
            "Epoch 510, training loss: 0.2708348581880813\n",
            "Epoch 511, training loss: 0.2708261832004772\n",
            "Epoch 512, training loss: 0.2708175107572588\n",
            "Epoch 513, training loss: 0.27080884085937457\n",
            "Epoch 514, training loss: 0.2708001735077719\n",
            "Epoch 515, training loss: 0.2707915087033983\n",
            "Epoch 516, training loss: 0.27078284644720063\n",
            "Epoch 517, training loss: 0.2707741867401255\n",
            "Epoch 518, training loss: 0.27076552958311884\n",
            "Epoch 519, training loss: 0.27075687497712614\n",
            "Epoch 520, training loss: 0.27074822292309286\n",
            "Epoch 521, training loss: 0.27073957342196364\n",
            "Epoch 522, training loss: 0.27073092647468294\n",
            "Epoch 523, training loss: 0.27072228208219473\n",
            "Epoch 524, training loss: 0.27071364024544214\n",
            "Epoch 525, training loss: 0.2707050009653686\n",
            "Epoch 526, training loss: 0.2706963642429165\n",
            "Epoch 527, training loss: 0.27068773007902797\n",
            "Epoch 528, training loss: 0.2706790984746448\n",
            "Epoch 529, training loss: 0.2706704694307082\n",
            "Epoch 530, training loss: 0.2706618429481589\n",
            "Epoch 531, training loss: 0.27065321902793743\n",
            "Epoch 532, training loss: 0.2706445976709835\n",
            "Epoch 533, training loss: 0.27063597887823665\n",
            "Epoch 534, training loss: 0.2706273626506356\n",
            "Epoch 535, training loss: 0.2706187489891192\n",
            "Epoch 536, training loss: 0.2706101378946251\n",
            "Epoch 537, training loss: 0.2706015293680912\n",
            "Epoch 538, training loss: 0.27059292341045427\n",
            "Epoch 539, training loss: 0.2705843200226511\n",
            "Epoch 540, training loss: 0.2705757192056177\n",
            "Epoch 541, training loss: 0.27056712096028984\n",
            "Epoch 542, training loss: 0.27055852528760244\n",
            "Epoch 543, training loss: 0.2705499321884904\n",
            "Epoch 544, training loss: 0.2705413416638878\n",
            "Epoch 545, training loss: 0.27053275371472835\n",
            "Epoch 546, training loss: 0.2705241683419451\n",
            "Epoch 547, training loss: 0.2705155855464709\n",
            "Epoch 548, training loss: 0.2705070053292378\n",
            "Epoch 549, training loss: 0.27049842769117766\n",
            "Epoch 550, training loss: 0.2704898526332216\n",
            "Epoch 551, training loss: 0.27048128015630013\n",
            "Epoch 552, training loss: 0.2704727102613435\n",
            "Epoch 553, training loss: 0.27046414294928145\n",
            "Epoch 554, training loss: 0.27045557822104294\n",
            "Epoch 555, training loss: 0.27044701607755667\n",
            "Epoch 556, training loss: 0.27043845651975074\n",
            "Epoch 557, training loss: 0.2704298995485527\n",
            "Epoch 558, training loss: 0.2704213451648895\n",
            "Epoch 559, training loss: 0.2704127933696879\n",
            "Epoch 560, training loss: 0.27040424416387343\n",
            "Epoch 561, training loss: 0.27039569754837195\n",
            "Epoch 562, training loss: 0.27038715352410814\n",
            "Epoch 563, training loss: 0.27037861209200637\n",
            "Epoch 564, training loss: 0.27037007325299045\n",
            "Epoch 565, training loss: 0.2703615370079838\n",
            "Epoch 566, training loss: 0.270353003357909\n",
            "Epoch 567, training loss: 0.27034447230368824\n",
            "Epoch 568, training loss: 0.2703359438462432\n",
            "Epoch 569, training loss: 0.2703274179864948\n",
            "Epoch 570, training loss: 0.2703188947253637\n",
            "Epoch 571, training loss: 0.2703103740637698\n",
            "Epoch 572, training loss: 0.27030185600263235\n",
            "Epoch 573, training loss: 0.27029334054287035\n",
            "Epoch 574, training loss: 0.27028482768540196\n",
            "Epoch 575, training loss: 0.2702763174311449\n",
            "Epoch 576, training loss: 0.2702678097810162\n",
            "Epoch 577, training loss: 0.27025930473593235\n",
            "Epoch 578, training loss: 0.2702508022968094\n",
            "Epoch 579, training loss: 0.27024230246456266\n",
            "Epoch 580, training loss: 0.2702338052401068\n",
            "Epoch 581, training loss: 0.27022531062435623\n",
            "Epoch 582, training loss: 0.27021681861822444\n",
            "Epoch 583, training loss: 0.27020832922262433\n",
            "Epoch 584, training loss: 0.2701998424384684\n",
            "Epoch 585, training loss: 0.2701913582666685\n",
            "Epoch 586, training loss: 0.27018287670813584\n",
            "Epoch 587, training loss: 0.27017439776378094\n",
            "Epoch 588, training loss: 0.27016592143451384\n",
            "Epoch 589, training loss: 0.27015744772124395\n",
            "Epoch 590, training loss: 0.27014897662488013\n",
            "Epoch 591, training loss: 0.2701405081463304\n",
            "Epoch 592, training loss: 0.27013204228650245\n",
            "Epoch 593, training loss: 0.2701235790463031\n",
            "Epoch 594, training loss: 0.27011511842663866\n",
            "Epoch 595, training loss: 0.27010666042841497\n",
            "Epoch 596, training loss: 0.27009820505253695\n",
            "Epoch 597, training loss: 0.2700897522999091\n",
            "Epoch 598, training loss: 0.2700813021714353\n",
            "Epoch 599, training loss: 0.2700728546680185\n",
            "Epoch 600, training loss: 0.27006440979056145\n",
            "Epoch 601, training loss: 0.27005596753996586\n",
            "Epoch 602, training loss: 0.27004752791713316\n",
            "Epoch 603, training loss: 0.2700390909229639\n",
            "Epoch 604, training loss: 0.27003065655835795\n",
            "Epoch 605, training loss: 0.2700222248242146\n",
            "Epoch 606, training loss: 0.2700137957214328\n",
            "Epoch 607, training loss: 0.27000536925091023\n",
            "Epoch 608, training loss: 0.26999694541354435\n",
            "Epoch 609, training loss: 0.26998852421023195\n",
            "Epoch 610, training loss: 0.26998010564186875\n",
            "Epoch 611, training loss: 0.2699716897093504\n",
            "Epoch 612, training loss: 0.26996327641357154\n",
            "Epoch 613, training loss: 0.26995486575542604\n",
            "Epoch 614, training loss: 0.26994645773580744\n",
            "Epoch 615, training loss: 0.2699380523556082\n",
            "Epoch 616, training loss: 0.26992964961572047\n",
            "Epoch 617, training loss: 0.2699212495170355\n",
            "Epoch 618, training loss: 0.26991285206044396\n",
            "Epoch 619, training loss: 0.2699044572468358\n",
            "Epoch 620, training loss: 0.2698960650771001\n",
            "Epoch 621, training loss: 0.2698876755521257\n",
            "Epoch 622, training loss: 0.26987928867280025\n",
            "Epoch 623, training loss: 0.269870904440011\n",
            "Epoch 624, training loss: 0.2698625228546444\n",
            "Epoch 625, training loss: 0.2698541439175862\n",
            "Epoch 626, training loss: 0.2698457676297216\n",
            "Epoch 627, training loss: 0.26983739399193474\n",
            "Epoch 628, training loss: 0.26982902300510947\n",
            "Epoch 629, training loss: 0.2698206546701286\n",
            "Epoch 630, training loss: 0.26981228898787446\n",
            "Epoch 631, training loss: 0.2698039259592283\n",
            "Epoch 632, training loss: 0.2697955655850713\n",
            "Epoch 633, training loss: 0.2697872078662833\n",
            "Epoch 634, training loss: 0.2697788528037437\n",
            "Epoch 635, training loss: 0.26977050039833095\n",
            "Epoch 636, training loss: 0.2697621506509231\n",
            "Epoch 637, training loss: 0.2697538035623973\n",
            "Epoch 638, training loss: 0.26974545913362996\n",
            "Epoch 639, training loss: 0.2697371173654967\n",
            "Epoch 640, training loss: 0.2697287782588726\n",
            "Epoch 641, training loss: 0.2697204418146317\n",
            "Epoch 642, training loss: 0.2697121080336473\n",
            "Epoch 643, training loss: 0.26970377691679254\n",
            "Epoch 644, training loss: 0.26969544846493904\n",
            "Epoch 645, training loss: 0.26968712267895806\n",
            "Epoch 646, training loss: 0.26967879955972024\n",
            "Epoch 647, training loss: 0.26967047910809494\n",
            "Epoch 648, training loss: 0.26966216132495135\n",
            "Epoch 649, training loss: 0.26965384621115746\n",
            "Epoch 650, training loss: 0.2696455337675808\n",
            "Epoch 651, training loss: 0.26963722399508794\n",
            "Epoch 652, training loss: 0.2696289168945447\n",
            "Epoch 653, training loss: 0.2696206124668163\n",
            "Epoch 654, training loss: 0.26961231071276687\n",
            "Epoch 655, training loss: 0.26960401163326003\n",
            "Epoch 656, training loss: 0.2695957152291586\n",
            "Epoch 657, training loss: 0.2695874215013244\n",
            "Epoch 658, training loss: 0.2695791304506188\n",
            "Epoch 659, training loss: 0.269570842077902\n",
            "Epoch 660, training loss: 0.2695625563840337\n",
            "Epoch 661, training loss: 0.26955427336987264\n",
            "Epoch 662, training loss: 0.26954599303627697\n",
            "Epoch 663, training loss: 0.2695377153841038\n",
            "Epoch 664, training loss: 0.2695294404142095\n",
            "Epoch 665, training loss: 0.2695211681274498\n",
            "Epoch 666, training loss: 0.2695128985246794\n",
            "Epoch 667, training loss: 0.26950463160675237\n",
            "Epoch 668, training loss: 0.2694963673745219\n",
            "Epoch 669, training loss: 0.2694881058288402\n",
            "Epoch 670, training loss: 0.269479846970559\n",
            "Epoch 671, training loss: 0.2694715908005289\n",
            "Epoch 672, training loss: 0.2694633373196\n",
            "Epoch 673, training loss: 0.26945508652862116\n",
            "Epoch 674, training loss: 0.26944683842844075\n",
            "Epoch 675, training loss: 0.26943859301990625\n",
            "Epoch 676, training loss: 0.2694303503038642\n",
            "Epoch 677, training loss: 0.2694221102811604\n",
            "Epoch 678, training loss: 0.26941387295263974\n",
            "Epoch 679, training loss: 0.2694056383191463\n",
            "Epoch 680, training loss: 0.2693974063815234\n",
            "Epoch 681, training loss: 0.2693891771406134\n",
            "Epoch 682, training loss: 0.26938095059725786\n",
            "Epoch 683, training loss: 0.26937272675229745\n",
            "Epoch 684, training loss: 0.26936450560657205\n",
            "Epoch 685, training loss: 0.26935628716092086\n",
            "Epoch 686, training loss: 0.2693480714161816\n",
            "Epoch 687, training loss: 0.26933985837319196\n",
            "Epoch 688, training loss: 0.26933164803278825\n",
            "Epoch 689, training loss: 0.26932344039580586\n",
            "Epoch 690, training loss: 0.2693152354630797\n",
            "Epoch 691, training loss: 0.2693070332354436\n",
            "Epoch 692, training loss: 0.26929883371373026\n",
            "Epoch 693, training loss: 0.2692906368987721\n",
            "Epoch 694, training loss: 0.2692824427914001\n",
            "Epoch 695, training loss: 0.2692742513924446\n",
            "Epoch 696, training loss: 0.2692660627027351\n",
            "Epoch 697, training loss: 0.2692578767231002\n",
            "Epoch 698, training loss: 0.2692496934543675\n",
            "Epoch 699, training loss: 0.26924151289736376\n",
            "Epoch 700, training loss: 0.26923333505291497\n",
            "Epoch 701, training loss: 0.26922515992184604\n",
            "Epoch 702, training loss: 0.2692169875049812\n",
            "Epoch 703, training loss: 0.26920881780314343\n",
            "Epoch 704, training loss: 0.2692006508171552\n",
            "Epoch 705, training loss: 0.26919248654783795\n",
            "Epoch 706, training loss: 0.269184324996012\n",
            "Epoch 707, training loss: 0.26917616616249707\n",
            "Epoch 708, training loss: 0.26916801004811175\n",
            "Epoch 709, training loss: 0.26915985665367387\n",
            "Epoch 710, training loss: 0.26915170598000016\n",
            "Epoch 711, training loss: 0.2691435580279067\n",
            "Epoch 712, training loss: 0.26913541279820835\n",
            "Epoch 713, training loss: 0.2691272702917192\n",
            "Epoch 714, training loss: 0.2691191305092525\n",
            "Epoch 715, training loss: 0.2691109934516204\n",
            "Epoch 716, training loss: 0.2691028591196342\n",
            "Epoch 717, training loss: 0.2690947275141043\n",
            "Epoch 718, training loss: 0.26908659863584\n",
            "Epoch 719, training loss: 0.26907847248565\n",
            "Epoch 720, training loss: 0.2690703490643416\n",
            "Epoch 721, training loss: 0.26906222837272153\n",
            "Epoch 722, training loss: 0.26905411041159544\n",
            "Epoch 723, training loss: 0.26904599518176797\n",
            "Epoch 724, training loss: 0.26903788268404294\n",
            "Epoch 725, training loss: 0.26902977291922303\n",
            "Epoch 726, training loss: 0.26902166588811033\n",
            "Epoch 727, training loss: 0.2690135615915056\n",
            "Epoch 728, training loss: 0.2690054600302087\n",
            "Epoch 729, training loss: 0.26899736120501855\n",
            "Epoch 730, training loss: 0.2689892651167333\n",
            "Epoch 731, training loss: 0.26898117176615\n",
            "Epoch 732, training loss: 0.2689730811540645\n",
            "Epoch 733, training loss: 0.26896499328127216\n",
            "Epoch 734, training loss: 0.2689569081485668\n",
            "Epoch 735, training loss: 0.2689488257567418\n",
            "Epoch 736, training loss: 0.2689407461065893\n",
            "Epoch 737, training loss: 0.2689326691989002\n",
            "Epoch 738, training loss: 0.26892459503446503\n",
            "Epoch 739, training loss: 0.2689165236140728\n",
            "Epoch 740, training loss: 0.2689084549385118\n",
            "Epoch 741, training loss: 0.2689003890085692\n",
            "Epoch 742, training loss: 0.2688923258250312\n",
            "Epoch 743, training loss: 0.2688842653886831\n",
            "Epoch 744, training loss: 0.2688762077003092\n",
            "Epoch 745, training loss: 0.2688681527606924\n",
            "Epoch 746, training loss: 0.2688601005706153\n",
            "Epoch 747, training loss: 0.268852051130859\n",
            "Epoch 748, training loss: 0.26884400444220363\n",
            "Epoch 749, training loss: 0.26883596050542846\n",
            "Epoch 750, training loss: 0.2688279193213116\n",
            "Epoch 751, training loss: 0.26881988089063025\n",
            "Epoch 752, training loss: 0.2688118452141606\n",
            "Epoch 753, training loss: 0.2688038122926778\n",
            "Epoch 754, training loss: 0.2687957821269559\n",
            "Epoch 755, training loss: 0.26878775471776784\n",
            "Epoch 756, training loss: 0.26877973006588596\n",
            "Epoch 757, training loss: 0.268771708172081\n",
            "Epoch 758, training loss: 0.2687636890371232\n",
            "Epoch 759, training loss: 0.2687556726617814\n",
            "Epoch 760, training loss: 0.26874765904682346\n",
            "Epoch 761, training loss: 0.2687396481930163\n",
            "Epoch 762, training loss: 0.26873164010112577\n",
            "Epoch 763, training loss: 0.26872363477191663\n",
            "Epoch 764, training loss: 0.2687156322061527\n",
            "Epoch 765, training loss: 0.2687076324045965\n",
            "Epoch 766, training loss: 0.26869963536800995\n",
            "Epoch 767, training loss: 0.2686916410971534\n",
            "Epoch 768, training loss: 0.2686836495927863\n",
            "Epoch 769, training loss: 0.2686756608556674\n",
            "Epoch 770, training loss: 0.26866767488655396\n",
            "Epoch 771, training loss: 0.26865969168620235\n",
            "Epoch 772, training loss: 0.26865171125536785\n",
            "Epoch 773, training loss: 0.26864373359480465\n",
            "Epoch 774, training loss: 0.2686357587052659\n",
            "Epoch 775, training loss: 0.26862778658750364\n",
            "Epoch 776, training loss: 0.26861981724226897\n",
            "Epoch 777, training loss: 0.2686118506703116\n",
            "Epoch 778, training loss: 0.2686038868723807\n",
            "Epoch 779, training loss: 0.2685959258492237\n",
            "Epoch 780, training loss: 0.2685879676015874\n",
            "Epoch 781, training loss: 0.2685800121302174\n",
            "Epoch 782, training loss: 0.26857205943585805\n",
            "Epoch 783, training loss: 0.26856410951925297\n",
            "Epoch 784, training loss: 0.2685561623811443\n",
            "Epoch 785, training loss: 0.2685482180222734\n",
            "Epoch 786, training loss: 0.26854027644338024\n",
            "Epoch 787, training loss: 0.26853233764520384\n",
            "Epoch 788, training loss: 0.2685244016284823\n",
            "Epoch 789, training loss: 0.2685164683939521\n",
            "Epoch 790, training loss: 0.26850853794234913\n",
            "Epoch 791, training loss: 0.268500610274408\n",
            "Epoch 792, training loss: 0.2684926853908622\n",
            "Epoch 793, training loss: 0.268484763292444\n",
            "Epoch 794, training loss: 0.2684768439798845\n",
            "Epoch 795, training loss: 0.26846892745391404\n",
            "Epoch 796, training loss: 0.2684610137152616\n",
            "Epoch 797, training loss: 0.26845310276465506\n",
            "Epoch 798, training loss: 0.26844519460282107\n",
            "Epoch 799, training loss: 0.2684372892304853\n",
            "Epoch 800, training loss: 0.26842938664837224\n",
            "Epoch 801, training loss: 0.26842148685720535\n",
            "Epoch 802, training loss: 0.2684135898577065\n",
            "Epoch 803, training loss: 0.26840569565059724\n",
            "Epoch 804, training loss: 0.26839780423659715\n",
            "Epoch 805, training loss: 0.26838991561642517\n",
            "Epoch 806, training loss: 0.268382029790799\n",
            "Epoch 807, training loss: 0.2683741467604352\n",
            "Epoch 808, training loss: 0.26836626652604895\n",
            "Epoch 809, training loss: 0.26835838908835447\n",
            "Epoch 810, training loss: 0.268350514448065\n",
            "Epoch 811, training loss: 0.26834264260589225\n",
            "Epoch 812, training loss: 0.26833477356254715\n",
            "Epoch 813, training loss: 0.2683269073187391\n",
            "Epoch 814, training loss: 0.2683190438751766\n",
            "Epoch 815, training loss: 0.2683111832325668\n",
            "Epoch 816, training loss: 0.26830332539161594\n",
            "Epoch 817, training loss: 0.2682954703530288\n",
            "Epoch 818, training loss: 0.26828761811750934\n",
            "Epoch 819, training loss: 0.2682797686857599\n",
            "Epoch 820, training loss: 0.26827192205848194\n",
            "Epoch 821, training loss: 0.26826407823637577\n",
            "Epoch 822, training loss: 0.2682562372201403\n",
            "Epoch 823, training loss: 0.2682483990104734\n",
            "Epoch 824, training loss: 0.26824056360807186\n",
            "Epoch 825, training loss: 0.26823273101363104\n",
            "Epoch 826, training loss: 0.26822490122784537\n",
            "Epoch 827, training loss: 0.2682170742514076\n",
            "Epoch 828, training loss: 0.2682092500850101\n",
            "Epoch 829, training loss: 0.2682014287293433\n",
            "Epoch 830, training loss: 0.2681936101850968\n",
            "Epoch 831, training loss: 0.26818579445295887\n",
            "Epoch 832, training loss: 0.26817798153361655\n",
            "Epoch 833, training loss: 0.2681701714277559\n",
            "Epoch 834, training loss: 0.2681623641360616\n",
            "Epoch 835, training loss: 0.2681545596592169\n",
            "Epoch 836, training loss: 0.26814675799790444\n",
            "Epoch 837, training loss: 0.268138959152805\n",
            "Epoch 838, training loss: 0.26813116312459856\n",
            "Epoch 839, training loss: 0.2681233699139637\n",
            "Epoch 840, training loss: 0.26811557952157783\n",
            "Epoch 841, training loss: 0.2681077919481171\n",
            "Epoch 842, training loss: 0.26810000719425653\n",
            "Epoch 843, training loss: 0.26809222526066984\n",
            "Epoch 844, training loss: 0.2680844461480295\n",
            "Epoch 845, training loss: 0.26807666985700684\n",
            "Epoch 846, training loss: 0.26806889638827186\n",
            "Epoch 847, training loss: 0.2680611257424934\n",
            "Epoch 848, training loss: 0.2680533579203389\n",
            "Epoch 849, training loss: 0.26804559292247493\n",
            "Epoch 850, training loss: 0.26803783074956633\n",
            "Epoch 851, training loss: 0.2680300714022771\n",
            "Epoch 852, training loss: 0.26802231488126965\n",
            "Epoch 853, training loss: 0.2680145611872055\n",
            "Epoch 854, training loss: 0.26800681032074475\n",
            "Epoch 855, training loss: 0.2679990622825461\n",
            "Epoch 856, training loss: 0.2679913170732673\n",
            "Epoch 857, training loss: 0.26798357469356454\n",
            "Epoch 858, training loss: 0.26797583514409307\n",
            "Epoch 859, training loss: 0.26796809842550645\n",
            "Epoch 860, training loss: 0.26796036453845745\n",
            "Epoch 861, training loss: 0.2679526334835972\n",
            "Epoch 862, training loss: 0.2679449052615759\n",
            "Epoch 863, training loss: 0.267937179873042\n",
            "Epoch 864, training loss: 0.2679294573186432\n",
            "Epoch 865, training loss: 0.2679217375990257\n",
            "Epoch 866, training loss: 0.26791402071483433\n",
            "Epoch 867, training loss: 0.2679063066667127\n",
            "Epoch 868, training loss: 0.2678985954553033\n",
            "Epoch 869, training loss: 0.2678908870812472\n",
            "Epoch 870, training loss: 0.26788318154518403\n",
            "Epoch 871, training loss: 0.2678754788477525\n",
            "Epoch 872, training loss: 0.26786777898958974\n",
            "Epoch 873, training loss: 0.2678600819713317\n",
            "Epoch 874, training loss: 0.267852387793613\n",
            "Epoch 875, training loss: 0.267844696457067\n",
            "Epoch 876, training loss: 0.2678370079623256\n",
            "Epoch 877, training loss: 0.2678293223100198\n",
            "Epoch 878, training loss: 0.26782163950077903\n",
            "Epoch 879, training loss: 0.2678139595352313\n",
            "Epoch 880, training loss: 0.2678062824140034\n",
            "Epoch 881, training loss: 0.26779860813772094\n",
            "Epoch 882, training loss: 0.26779093670700815\n",
            "Epoch 883, training loss: 0.267783268122488\n",
            "Epoch 884, training loss: 0.267775602384782\n",
            "Epoch 885, training loss: 0.2677679394945104\n",
            "Epoch 886, training loss: 0.2677602794522923\n",
            "Epoch 887, training loss: 0.2677526222587452\n",
            "Epoch 888, training loss: 0.26774496791448543\n",
            "Epoch 889, training loss: 0.2677373164201281\n",
            "Epoch 890, training loss: 0.2677296677762869\n",
            "Epoch 891, training loss: 0.2677220219835739\n",
            "Epoch 892, training loss: 0.26771437904260054\n",
            "Epoch 893, training loss: 0.2677067389539762\n",
            "Epoch 894, training loss: 0.2676991017183093\n",
            "Epoch 895, training loss: 0.2676914673362068\n",
            "Epoch 896, training loss: 0.26768383580827443\n",
            "Epoch 897, training loss: 0.26767620713511664\n",
            "Epoch 898, training loss: 0.26766858131733623\n",
            "Epoch 899, training loss: 0.267660958355535\n",
            "Epoch 900, training loss: 0.2676533382503131\n",
            "Epoch 901, training loss: 0.26764572100226963\n",
            "Epoch 902, training loss: 0.2676381066120022\n",
            "Epoch 903, training loss: 0.267630495080107\n",
            "Epoch 904, training loss: 0.2676228864071789\n",
            "Epoch 905, training loss: 0.26761528059381157\n",
            "Epoch 906, training loss: 0.2676076776405971\n",
            "Epoch 907, training loss: 0.26760007754812626\n",
            "Epoch 908, training loss: 0.2675924803169887\n",
            "Epoch 909, training loss: 0.26758488594777236\n",
            "Epoch 910, training loss: 0.26757729444106404\n",
            "Epoch 911, training loss: 0.2675697057974492\n",
            "Epoch 912, training loss: 0.2675621200175117\n",
            "Epoch 913, training loss: 0.2675545371018342\n",
            "Epoch 914, training loss: 0.26754695705099796\n",
            "Epoch 915, training loss: 0.26753937986558285\n",
            "Epoch 916, training loss: 0.2675318055461675\n",
            "Epoch 917, training loss: 0.26752423409332887\n",
            "Epoch 918, training loss: 0.2675166655076428\n",
            "Epoch 919, training loss: 0.2675090997896836\n",
            "Epoch 920, training loss: 0.26750153694002443\n",
            "Epoch 921, training loss: 0.2674939769592365\n",
            "Epoch 922, training loss: 0.26748641984789034\n",
            "Epoch 923, training loss: 0.26747886560655454\n",
            "Epoch 924, training loss: 0.2674713142357968\n",
            "Epoch 925, training loss: 0.2674637657361828\n",
            "Epoch 926, training loss: 0.2674562201082773\n",
            "Epoch 927, training loss: 0.2674486773526436\n",
            "Epoch 928, training loss: 0.2674411374698436\n",
            "Epoch 929, training loss: 0.2674336004604375\n",
            "Epoch 930, training loss: 0.2674260663249844\n",
            "Epoch 931, training loss: 0.267418535064042\n",
            "Epoch 932, training loss: 0.26741100667816636\n",
            "Epoch 933, training loss: 0.26740348116791246\n",
            "Epoch 934, training loss: 0.2673959585338335\n",
            "Epoch 935, training loss: 0.26738843877648166\n",
            "Epoch 936, training loss: 0.2673809218964072\n",
            "Epoch 937, training loss: 0.2673734078941596\n",
            "Epoch 938, training loss: 0.2673658967702863\n",
            "Epoch 939, training loss: 0.2673583885253338\n",
            "Epoch 940, training loss: 0.26735088315984684\n",
            "Epoch 941, training loss: 0.2673433806743689\n",
            "Epoch 942, training loss: 0.267335881069442\n",
            "Epoch 943, training loss: 0.2673283843456069\n",
            "Epoch 944, training loss: 0.2673208905034025\n",
            "Epoch 945, training loss: 0.26731339954336675\n",
            "Epoch 946, training loss: 0.26730591146603583\n",
            "Epoch 947, training loss: 0.26729842627194467\n",
            "Epoch 948, training loss: 0.26729094396162667\n",
            "Epoch 949, training loss: 0.2672834645356138\n",
            "Epoch 950, training loss: 0.2672759879944366\n",
            "Epoch 951, training loss: 0.2672685143386242\n",
            "Epoch 952, training loss: 0.2672610435687043\n",
            "Epoch 953, training loss: 0.2672535756852031\n",
            "Epoch 954, training loss: 0.26724611068864523\n",
            "Epoch 955, training loss: 0.2672386485795543\n",
            "Epoch 956, training loss: 0.2672311893584519\n",
            "Epoch 957, training loss: 0.26722373302585856\n",
            "Epoch 958, training loss: 0.2672162795822931\n",
            "Epoch 959, training loss: 0.2672088290282732\n",
            "Epoch 960, training loss: 0.26720138136431476\n",
            "Epoch 961, training loss: 0.26719393659093255\n",
            "Epoch 962, training loss: 0.2671864947086395\n",
            "Epoch 963, training loss: 0.2671790557179473\n",
            "Epoch 964, training loss: 0.26717161961936614\n",
            "Epoch 965, training loss: 0.2671641864134047\n",
            "Epoch 966, training loss: 0.2671567561005704\n",
            "Epoch 967, training loss: 0.26714932868136876\n",
            "Epoch 968, training loss: 0.2671419041563043\n",
            "Epoch 969, training loss: 0.26713448252587974\n",
            "Epoch 970, training loss: 0.26712706379059636\n",
            "Epoch 971, training loss: 0.2671196479509542\n",
            "Epoch 972, training loss: 0.2671122350074516\n",
            "Epoch 973, training loss: 0.2671048249605855\n",
            "Epoch 974, training loss: 0.26709741781085117\n",
            "Epoch 975, training loss: 0.26709001355874273\n",
            "Epoch 976, training loss: 0.26708261220475266\n",
            "Epoch 977, training loss: 0.26707521374937176\n",
            "Epoch 978, training loss: 0.2670678181930895\n",
            "Epoch 979, training loss: 0.267060425536394\n",
            "Epoch 980, training loss: 0.2670530357797717\n",
            "Epoch 981, training loss: 0.26704564892370763\n",
            "Epoch 982, training loss: 0.2670382649686853\n",
            "Epoch 983, training loss: 0.2670308839151866\n",
            "Epoch 984, training loss: 0.2670235057636921\n",
            "Epoch 985, training loss: 0.2670161305146809\n",
            "Epoch 986, training loss: 0.26700875816863034\n",
            "Epoch 987, training loss: 0.2670013887260164\n",
            "Epoch 988, training loss: 0.26699402218731366\n",
            "Epoch 989, training loss: 0.2669866585529952\n",
            "Epoch 990, training loss: 0.26697929782353225\n",
            "Epoch 991, training loss: 0.2669719399993948\n",
            "Epoch 992, training loss: 0.2669645850810514\n",
            "Epoch 993, training loss: 0.26695723306896896\n",
            "Epoch 994, training loss: 0.2669498839636128\n",
            "Epoch 995, training loss: 0.26694253776544685\n",
            "Epoch 996, training loss: 0.2669351944749335\n",
            "Epoch 997, training loss: 0.2669278540925336\n",
            "Epoch 998, training loss: 0.2669205166187064\n",
            "Epoch 999, training loss: 0.26691318205390985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ggZy31Hn67P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b339db-1eff-4da1-c1d0-2a6d3a3622e7"
      },
      "source": [
        "sample_size = 20\n",
        "\n",
        "def sample(params,ch2id,id2ch,vocab_size, hidden_size, sample_size):\n",
        "  W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = params\n",
        "\n",
        "  sample_string = \"\"\n",
        "\n",
        "  h = np.zeros((hidden_size,1))\n",
        "  c = np.zeros((hidden_size,1))\n",
        "  h_prev  = h\n",
        "  C_prev = c\n",
        "\n",
        "  x = np.zeros((vocab_size,1))\n",
        "\n",
        "  for t in range(sample_size):\n",
        "    z = np.row_stack((h_prev,x))\n",
        "    f = sigmoid(np.dot(W_f, z) + b_f)\n",
        "    i = sigmoid(np.dot(W_i,z) + b_i)\n",
        "    g = tanh(np.dot(W_g, z) + b_g)\n",
        "    C_prev = C_prev * f + i * g\n",
        "    o = sigmoid(np.dot(W_o,z) + b_o)\n",
        "    h_prev = o * tanh(C_prev)\n",
        "    v = np.dot(W_v, h_prev) + b_v  \n",
        "    output = softmax(v)\n",
        "\n",
        "    # print(t)\n",
        "\n",
        "    idx = np.random.choice(range(vocab_size), p = np.ravel(output))\n",
        "    # idx = np.argmax(output)\n",
        "    x = np.zeros((vocab_size,1))\n",
        "    x[idx] = 1\n",
        "\n",
        "    sample_string += id2ch[idx] + \" \"\n",
        "    # print(sample_string)\n",
        "\n",
        "  return sample_string\n",
        "\n",
        "\n",
        "s = sample(params,ch2id,id2ch,vocab_size, hidden_size, sample_size)\n",
        "print(s)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gladly hear'st strong time bud attending face cruel: self-killed: which see never-resting light where sweetly make gaze grave prime singleness \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zvOF_hT0LCkY",
        "outputId": "55919209-2d9d-44ce-dad9-f89fe4c3cc7b"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"gazed consum'st heir head willing thine climbed homage tillage prime another where under bounteous highmost through shame loan; widow's left \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF3XKTcLEhZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}